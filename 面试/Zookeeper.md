# Zookeeper

1.Zookeeper简介

Zookeeper是一个开源的分布式协调服务，设计目的是将复杂并且容易出错的的分布式一致性服务封装起来，构成一个高效可靠的原语集(具有不可分割性，执行过程
不可中断)，并以一系列简单易用的接口提供给用户使用，Zookeeper提供高可用，高性能，稳定的分布式数据一致性解决方法，通常被用于实现诸如数据发布/订阅，
负载均衡，命名服务，分布式协调/通知，集群管理，Master选举，分布式锁和分布式队列等。同时Zookeeper将数据保存在内存中，读取效率高。

Zookeeper的特点是顺序一致性，原子性，单一系统映像和可靠性，顺序一致性是指从同一个客户端发起的请求最终会严格按照顺序被应用到Zookeeper中去，
原子性是指所有事务请求的处理结果在集群上的所有机器是一样的，要么成功，要么失败。单一系统映像是指客户端连接任一Zookeeper节点看到的数据模型都是一致的。
可靠性是指请求处理的结果会被持久化。

Zookeeper主要可以应用于分布式锁，命名服务和数据发布/订阅等，分布式锁可以根据在Zookeeper上创建唯一节点来获取分布式锁，执行完成，客户端宕机或者Zookeeper
宕机就会释放锁。命名服务可以通过Zookeeper的顺序节点生成全局唯一ID，分布式自增ID。数据发布/订阅通过watcher机制可以实现数据发布/订阅，通过将数据发布到
Zookeeper监听的节点上，其他节点通过监听节点数据的变化来实现配置的动态更新。

2.Zookeeper数据结构

Zookeeper的数据模型采用层次化的多叉树结构，类似Unix的文件系统路径，每个节点上都可以存储数据，数据可以是数字，字符串或者是二进制序列，每个节点都拥有N个子节点，
最上层的根节点用"/"来表示，每个数据节点称为Znode，是Zookeeper中数据的最小单位，每个Znode都有一个唯一的路径标识。Zookeeper主要是用来协调服务的，而不是用来
存储数据的，所以Zookeeper中每个Znode的数据存储上限是1M。Znode主要分为4中类型，持久节点，临时节点，持久顺序节点和临时顺序节点，持久(PERSISTENT)节点是创建
成功就持久化，即使Zookeeper集群宕机也存在，直到手动删除节点。临时(EPHEMERAL)节点是创建的节点的生命周期和客户端会话(session)绑定，会话销毁则节点消失，临时
节点只能做叶子节点，不能创建子节点。持久顺序(PERSISTENT_SEQUENTIAL)节点是具有顺序的持久节点，即节点的名字按照创建的顺序递增，/node1/app0000000001，
/node1/app0000000002。临时顺序(EPHEMERAL_SEQUENTIAL)节点是具有顺序的临时节点。Znode存储两部分数据，分别是stat和data，stat存储Znode的节点状态信息，
包括cZxid(create ZXID，数据节点被创建的事务id)，ctime(create time，节点创建的时间)，mZxid(modified ZXID，数据节点最后一次更新的事务id)，mtime(modified time，
节点的更新时间)，pZxid(节点的子节点列表的最后一次更新的事务id，只有子节点结构改变会更新，子节点数据改变不会更新)，cversion(子节点的版本号，当前节点的子节点
每次变化则加1)，dataVersion(数据节点内容的版本号，节点创建时为0，每更新一次节点内容则加1)，aclVersion(节点的ACL版本号，表示节点ACL信息变更的次数)，
ephemeralOwner(创建临时节点的会话sessionId，如果当前节点为持久节点，则为0)，dataLength(数据节点的内容长度)，numChildren(当前节点的子节点个数)。

Znode的ACL是指Zookeeper的权限控制(AccessControlLists)，类似UNIX的文件系统权限控制，主要有5种，CREATE，READ，WRITE，DELETE，ADMIN。CREATE能够创建子节点，
READ能够获取节点数据和子节点，WRITE设置或更新节点数据，DELETE删除子节点，ADMIN设置节点ACL的权限。CREATE和DELETE都是针对子节点的权限。对于身份认证提供4种方式，
world，auth，digest和ip，world是默认方法，所有用户都可访问，auth任何认证的用户可以访问，digest使用用户名和密码验证，ip对指定ip进行限制。

3.Zookeeper的Watcher

Zookeeper的Watcher(时间监听器)是指Zookeeper允许用户在指定节点上注册一些Watcher，并且在特定事件触发后，Zookeeper服务端会将事件通知给用户去进行回调处理。

![timewoo](https://timewoo.github.io/images/Zookeeper-Watcher.png)

4.Zookeeper的Session

Session是Zookeeper服务器和客户端之间的一个TCP长连接，通过这个连接客户端可以通过心跳检测和服务器保持会话，也能向服务器发送请求并响应，还能够接受来自服务器的
Watcher事件通知。Session的sessionTimeout(会话超时时间)，当服务器压力过大，网络故障，客户端主动断开连接等原因导致客户端断开时，只要在sessionTimeout规定时间
内连接上集群的任一服务器，之前创建的会话仍然有效。同时服务端会给每个客户端分配一个唯一的sessionId，由于客户端的许多操作基于sessionId，需要保证全局唯一。

5.Zookeeper集群

Zookeeper为了保证高可用采用集群的形式部署，最少可以使用3台服务器构成集群，Zookeeper服务器在内存中维护当前服务器的状态，每台服务器之间保持通信，集群间通过
ZAB协议(Zookeeper Atomic Broadcast)来保证数据一致性。一般最经典的集群模式是采用Master/Slave模式，即主备模式，Master服务器作为主服务器提供写操作，其他
的Slave服务器作为从服务器通过异步复制Master数据提供读操作。Zookeeper并没有采用这种模式，而是引入了Leader，Follower和Observer三种角色，Leader是为客户端提供
读写服务，负责投票的发起和决议，更新系统状态。Follower是为客户端提供读服务，将写操作转发给Leader，在选举中参与投票。Observer是为客户端提供读服务，将写操作转发给
Leader，不参与选举投票，也不参与"过半写成功"策略，主要是在不影响写性能下提升集群的读性能，Zookeeper3.3新增角色。

Zookeeper选举是指当Leader服务器出现异常导致无法连接时，会进入Leader选举过程，这个过程会选举新的Leader服务器。主要有四个阶段，leader election，Discovery，
Synchronization和Broadcast，leader election(选举阶段)是所有节点在一开始都处于选举阶段，只要有一个节点得到超过半数的节点的票数，就可以选举为新的leader，
Discovery(发现阶段)是Follower节点和准备成为Leader的节点通信，同步Follower节点最近接受的事务提议。Synchronization(同步阶段)是准备成为Leader的节点将在Discovery
阶段获取的历史事务提议同步到集群的所有副本中，同步完成后准Leader成为真正的Leader节点。Broadcast(广播阶段)是Zookeeper集群正式对外提供事务服务，并且leader节点进行
消息广播，同时有新节点加入也会对新节点进行同步。

![timewoo](https://timewoo.github.io/images/Zookeeper-Role.png)

Zookeeper集群服务器状态有LOOKING，LEADING，FOLLOWING，OBSERVING，LOOKING是正在寻找leader状态，LEADING是leader状态，对应是leader节点，FOLLOWING是follower状态，
对应是follower节点，OBSERVING是observer状态，对应是Observer节点，不参与选举。

Zookeeper底层算法有ZAB协议和Paxos算法，Paxos算法主要解决的是分布式一致性的问题，ZAB(Zookeeper Atomic Broadcast 原子广播)是Zookeeper专门设计的一种支持崩溃恢复的
原子广播协议。ZAB协议是Zookeeper在Paxos算法的基础上特定实现的一种分布式数据一致性协议。基于ZAB协议，Zookeeper实现一种主备模式的架构来保证集群中的各个副本之间的数据
一致性。由于Zookeeper是一个分布式集群架构，所以必须满足CAP协议中的两种。CAP分布式协议是指在分布式架构中需要满足的三个条件，C是数据一致性(consistency)，即保证分布式中
的多个副本之间在同一时刻有相同的值。A是系统可用性(availability)，即在部分节点出现问题时也能够对外提供服务。P是分区容忍性(partition tolerance)，即部分节点出现网络分区
(节点之间无法)时也需要对外提供服务。由于CAP无法同时满足，P必须满足，所以分布式集群需要满足AP或CP，Eureka实现的是AP，Zookeeper实现的CP，保证分布式数据一致性。
实现CP的协议和算法主要有2PC，3PC和Paxos算法。

2PC是两阶段提交，主要运用于数据库的分布式事务的处理。两阶段提交中主要涉及两个角色，协调者和参与者，第一阶段是事务的发起者向协调者发起事务请求，协调者向所有参与者发送
prepare请求(包含事务内容)，参与者执行事务内容完成后不提交事务，将undo和redo信息计入日志中，然后给协调者反馈事务执行情况。第二阶段协调者根据参与者的反馈情况来判断给
参与者发送事务提交还是回滚请求，参与者收到请求执行之后给协调者反馈执行情况，协调者给事务发起者返回事务执行信息。2PC缺点是单点故障(协调者宕机则系统无法运行)，
阻塞问题(协调者发送prepare请求，参与者执行事务后不提交会占用资源，如果协调者宕机则资源无法释放)，数据不一致问题(协调者给参与者发送事务提交请求时宕机，部分事务提交，导致数据不一致)。

3PC是三阶段提交，主要解决2PC中出现的问题，3PC主要是CanCommit阶段，PreCommit阶段和DoCommit阶段，也存在协调者和参与者。CanCommit阶段是协调者向所有参与者发送CanCommit请求，
参与者根据自身是否能够执行事务向协调者反馈，可以返回Yes并且进入预备状态，否则返回No。PreCommit阶段协调者根据参与者的反馈来进行处理，若所有参与者返回YES，则协调者向所有参与者
发送PreCommit预提交请求，参与者收到PreCommit请求后进行事务执行，并将Undo和Redo日志信息写入事务日志中，执行成功后参与者反馈事务执行情况。若CanCommit阶段有参与者返回NO或者在
规定时间内有参与者未响应请求，则协调者会向所有参与者发送abort请求(中断请求)，执行事务中断处理，若参与者返回YES并且在规定时间内未收到PreCommit请求，则也会执行abort请求。
DoCommit阶段是协调者收到所有参与者的PreCommit请求的YES反馈后，会给所有参与者发送DoCommit请求，参与者收到请求后会进行事务提交，完成后给协调者反馈，协调者收到所有参与者的反馈后完成事务。
若有参与者返回NO或响应超时，则协调者会给所有参与者发送abort请求，参与者收到abort请求后利用undo日志回滚事务，完成回滚后给协调者发送反馈信息，协调者收到所有反馈后事务中断。
同时在DoCommit阶段若有参与者未收到协调者事务提交请求则会在一定时间内提交事务，因为3PC认为所有进入DoCommit阶段的参与者都是通过了CanCommit阶段，都是可以进行事务的提交。
由于有超时提交机制，所以参与者不会一直占用资源。虽然3PC解决了2PC单点故障和阻塞问题，但是还是会出现一致性问题，即abort请求无法到达的参与者会进行超时事务提交，就会导致出现一致性问题。

Paxos算法是基于消息传递的具有高度容错特性的一致性算法，是目前公认的解决分布式一致性问题的最有效的算法之一，解决分布式系统中如何就某个值达成一致。Paxos中主要
有三种角色，Proposer提案者，Acceptor表决者和Learner学习者。Paxos和2PC一样也有两个阶段，prepare和accept。prepare阶段中Proposer负责提出proposal，每个Proposer在提出提案时
会获取一个全局唯一递增的提案编号N，Proposer将N发送给所有的Acceptor，Acceptor在本地保存accept阶段的提交的提案编号N，因此Acceptor可以在本地找到N最大的提案MaxN，当Acceptor收到
Proposer发送的N时会和本地的MaxN比较，若MaxN==null则反馈Acceptor的节点编号，若MaxN>N则不回应或回复错误。若MaxN<N则反馈Acceptor本地accept的MaxN编号的提案内容。accept阶段是
当Proposer收到半数以上的Acceptor的批准，那么Proposer会给所有的Acceptor发送提案编号和内存。Acceptor收到提案编号和内容后和Acceptor的本地的MaxN比较，如果收到的N>=MaxN则accept
提案，反馈给Proposer，不满足则返回错误或不返回。当Proposer收到半数以上accept请求的反馈时会给所有的Acceptor发送提交请求，对于没有返回accept反馈的Acceptor，Proposer会发送提案
编号和内容进行执行和提交。若Proposer没有收到半数以上的accept请求的反馈时将对提案编号递增，然后重新进入Prepare阶段。Paxos算法会出现死循环问题，当两个Proposer在prepare阶段先后
向Acceptor发送提案N1和N2，Acceptor通过后反馈信息，本地的MaxN=N2，当N1的Proposer进行accept阶段时，由于Acceptor的本地MaxN=N2>N1，所以N1的Proposer的accept阶段不成功，Proposer
对N1进行自增变成N3重复Prepare阶段，Acceptor的本地MaxN=N3，然后N2的Proposer的accept阶段也不成功，自增后重复Prepare阶段，导致Proposer持续自增陷入死循环。解决方法是选定只有一个
proposer可以对Acceptor提出提案。

![timewoo](https://timewoo.github.io/images/paxos.png)

ZAB是Zookeeper自己定制实现的一致性协议，ZAB中主要有Leader领导者，Follower跟随者和Observer观察者。Leader是集群中唯一的处理写请求的处理者，能够
发起投票，Follower能够处理客户端读请求，写请求会转发给Leader处理，在选举过程中参与投票，有选举和被选举权。Observer就是没有选举和被选举权的Follower。ZAB协议中有消息广播和崩溃恢复
模式。消息广播模式主要应用于保证Leader，Follower和Observer之间的一致性，第一步是leader将写请求广播到所有的Follower，若超过半数的Follower同意则进行Follower和Observer的更新。
Leader为了保证写请求的顺序性在内部维护了一个请求的队列，同时生成全局唯一的自增ZXID来保证写请求是顺序执行的。崩溃恢复模式是指当Leader节点宕机需要重新选举或者Zookeeper启动初始化时保证集群
的数据一致性，当选举产生新的leader的节点，同时集群中半数以上的Follower节点和leader节点完成同步后会退出该模式。为了确保在选举新节点之后集群数据一致，需要让Leader提交的提案最终
被所有Follower提交和跳过丢弃的提案。当Leader在发送提交请求给部分Follower时宕机，可能选举未提交的Follower成为新leader，就会出现数据不一致，因此在选举新的Leader时要选举ZXID较大
的。还有一种情况是当leader还未将提交请求发送时宕机，新选举的leader没有原来leader提交的数据，当原来的leader恢复成为Follower时会比新的leader多出数据，但是由于集群其他节点并没有此数据，
所以会丢弃该提案。同时在Zookeeper进行leader选举的过程中是停止对外服务的，因为需要保证集群中数据的强一致性。



