## 缓存学习

## 缓存的使用用途：高性能(提升查询的效率)和高并发(提升请求的负载)。

## 缓存模型：
### redis和memcached区别：

#### 1.redis支持复杂的数据结构：
redis主要有string，hash，list，set，sorted set数据类型，string是最简单的存储结构，普通的get，set方法，做最简单的KV缓存。
hash类似map的数据结构，将对象数据缓存在redis，可以操作对象中的某个字段。list有序列表，可以存储列表型数据，还可以通过lrange命令实现分页查询，
类似下拉分页的效果，性能高。还可以实现简单的消息队列。set是无序的集合，自动去重，可以实现全局的redis的去重，还可以基于set进行交集，并集，差集等操作。
sorted set是排序的set，可以在写入redis时写入分数，自动根据分数排序存储。

#### 2.redis原生支持集群模式：
在redis3.x版本中支持集群模式，但是memcached没有原生的集群模式，需要依靠客户端来实现往集群中分片写入数据。

#### 3.redis在小存储中性能高
由于redis是单线程工作模型，使用单核，而memcached是多核，所以平均每个核上redis存储小数据时比memcached性能更高。而在100k以上的数据，memcached的性能要高于redis。

### redis的线程模型：
redis内部使用文件事件处理器file event handler，这个文件事件处理器是单线程的，所以redis才是单线程模型。它采用 IO 多路复用机制同时监听多个 socket，
将产生事件的 socket 压入内存队列中，事件分派器根据 socket 上的事件类型来选择对应的事件处理器进行处理。多路指的是多个客户端连接，复用是指单进程同时处理多个客户端的连接
，避免了线程上下文切换造成的消耗，传统的阻塞IO在资源阻塞时无法对外提供服务，redis是单线程，当一个连接阻塞时其他客户端将无法访问，所以采用IO 多路复用，即监听多个客户端连接，
当连接发生变化后，比如请求读写时，会将连接放入队列中，顺序请求处理，单线程避免了上下文切换，同时不会阻塞其他客户端的请求，也不需要轮询客户端获取请求，
文件事件处理器包括4个结构：多个socket，IO多路复用，文件事件分派器，事件处理器（连接应答处理器，命令请求处理器，命令回复处理器），多个 socket 可能会并发产生不同的操作，
每个操作对应不同的文件事件，但是 IO 多路复用程序会监听多个 socket，会将产生事件的 socket 放入队列中排队，事件分派器每次从队列中取出一个 socket，
根据 socket 的事件类型交给对应的事件处理器进行处理。redis客户端通信过程：

![timewoo](https://timewoo.github.io/images/redis.png)

首先，redis 服务端进程初始化的时候，会将 server socket 的 AE_READABLE 事件与连接应答处理器关联。客户端 socket01 向 redis 进程的 server socket 请求建立连接，
此时 server socket 会产生一个 AE_READABLE 事件，IO 多路复用程序监听到 server socket 产生的事件后，将该 socket 压入队列中。文件事件分派器从队列中获取 socket，
交给连接应答处理器。连接应答处理器会创建一个能与客户端通信的 socket01，并将该 socket01 的 AE_READABLE 事件与命令请求处理器关联。假设此时客户端发送了一个 set key value 请求，
此时 redis 中的 socket01 会产生 AE_READABLE 事件，IO 多路复用程序将 socket01 压入队列，此时事件分派器从队列中获取到 socket01 产生的 AE_READABLE 事件，
由于前面 socket01 的 AE_READABLE 事件已经与命令请求处理器关联，因此事件分派器将事件交给命令请求处理器来处理。命令请求处理器读取 socket01 的 key value 并在自己内存中
完成 key value 的设置。操作完成后，它会将 socket01 的 AE_WRITABLE 事件与命令回复处理器关联。如果此时客户端准备好接收返回结果了，那么 redis 中的 socket01 会产生一个
 AE_WRITABLE 事件，同样压入队列中，事件分派器找到相关联的命令回复处理器，由命令回复处理器对 socket01 输入本次操作的一个结果，比如 ok，之后解除 socket01 的 AE_WRITABLE 
 事件与命令回复处理器的关联。redis完成一次通信。AE_READABLE是当客户端对Socket执行read操作时产生的，AE_WRITABLE是当客户端对Socket执行write操作，close操作或者connect操作时产生。
 当一个Socket(套接字)产生两种事件时，会优先处理AE_READABLE，AE_READABLE处理完成后才执行AE_WRITABLE。

### redis效率：
1.纯内存操作，2.核心是基于非阻塞的IO多路复用机制，3.C 语言实现，一般来说，C 语言实现的程序“距离”操作系统更近，执行速度相对会更快。
4.单线程反而避免了多线程的频繁上下文切换问题，预防了多线程可能产生的竞争问题。

### redis过期策略：
redis过期策略是定期删除和惰性删除，定期删除就是redis默认100ms就随机抽取一些设置了过期时间的key，检查是否过期，过期就删除。重点是随机抽取，
并不是遍历key，这样在数据量大时能够减轻redis的压力。但是定期删除可能会导致时间到期的key没有被删除，就用到了惰性删除，就是在获取key时再去判断key是否过期，
如果过期就删除数据，同时不返回数据。还是有问题就是如果有数据没有定期删除+惰性删除，就会使内存的数据堆积，消耗内存。所以需要采用内存淘汰机制，主要有：
1.noeviction: 当内存不足以容纳新写入数据时，新写入操作会报错。2.allkeys-lru：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的 key。
3.allkeys-random：当内存不足以容纳新写入数据时，在键空间中，随机移除某个 key。4.volatile-lru：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，
移除最近最少使用的 key。5.volatile-random：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，随机移除某个 key。6.volatile-ttl：当内存不足以容纳新写入数据时，
在设置了过期时间的键空间中，有更早过期时间的 key 优先移除。基本使用hashMap+双向链表的结构去实现LRU(Least Recently Used)。
4.0之后添加了volatile-lfu(从已设置了过期时间的数据集中选择最不经常使用的数据淘汰)和allkeys-lfu(当内存空间不足以容纳新写入数据时，在键空间移除最不经常使用的key)

### redis高并发和高可用：
#### redis高并发：
##### redis主要依靠主从架构，一主多从，主节点用来写入数据，并且复制到从节点，从节点用来读数据。在实现高并发的同时可以使用集群，提供大容量的数据。
##### redis主从架构：

1.redis采用异步方式复制数据到从节点，redis2.8以上从节点会周期性确认每次复制的数据量。2.一个master节点可以配置多个slave节点。3.slave节点可以连接其他slave节点。
4.slave节点复制时，不会阻塞master节点的正常工作。5.slave节点在复制的时候不会阻塞对当前节点的查询操作，它会用旧数据提供服务，然后在复制完成之后就会删除旧数据，
加载新数据，这个时候就会停止服务。6.slave节点主要用来进行横向扩容，做读写分离，扩容的slave节点可以提高读的吞吐量。如果采用主从架构，建议开启master节点的持久化，
这样就能够避免master节点宕机后复制空数据到slave节点。

##### redis主从复制原理：
当启动一个slave节点的时候，它会发送一个PSYNC命令给master节点，如果是slave节点第一次连接master节点，就会触发full resynchronization(全量复制)。
此时master就会启动一个后台进程，生成一份RDB快照。同时会将从客户端新接受的写命令缓存在内存中。RDB文件生成完毕后master节点会将这个RDB发送给slave节点。
slave节点会先写入本地磁盘，然后在从本地磁盘加载到内存中，接着master节点将内存中缓存的数据发送到slave节点，slave也会同步这些数据，
如果slave节点和master节点断开连接之后，自动重连之后master节点只会复制slave缺失的数据。

##### 主从复制断点续传：
从redis2.8开始，如果主从复制过程中断，重新连接之后会继续从断开的地方重新复制，因为master节点会在内存中维护一个backlog，
master节点和slave节点会保存一个replica offset和master run id，replica offset保存在backlog中，如果复制中断，重新连接之后就会从offset的位置开始继续复制，
如果没有找到offset就重新开始一次resynchronization。redis可以设置无磁盘化复制，即master节点直接在内存中创建RDB，直接复制到slave中，
只需要配置文件中开启repl-diskless-sync yes。slave节点不会处理过期key，master节点在删除过期key之后就会给slave节点发送del命令，同步数据。

##### redis主从复制的完整流程：
slave node 启动时，会在自己本地保存 master node 的信息，包括 master node 的host和ip，但是复制流程没开始。slave node 内部有个定时任务，
每秒检查是否有新的 master node 要连接和复制，如果发现，就跟 master node 建立 socket 网络连接。然后 slave node 发送 ping 命令给 master node。
如果 master 设置了 requirepass，那么 slave node 必须发送 masterauth 的口令过去进行认证。master node 第一次执行全量复制，将所有数据发给 slave node。而在后续，
master node 持续将写命令，异步复制给 slave node。redis全量复制：master 执行 bgsave ，在本地生成一份 rdb 快照文件。master node 将 rdb 快照文件发送给 slave node，
如果 rdb 复制时间超过 60秒（repl-timeout），那么 slave node 就会认为复制失败，可以适当调大这个参数(对于千兆网卡的机器，一般每秒传输 100MB，6G 文件，很可能超过 60s)
master node 在生成 rdb 时，会将所有新的写命令缓存在内存中，在 slave node 保存了 rdb 之后，再将新的写命令复制给 slave node。如果在复制期间，
内存缓冲区持续消耗超过 64MB，或者一次性超过 256MB，那么停止复制，复制失败。slave node 接收到 rdb 之后，清空自己的旧数据，然后重新加载 rdb 到自己的内存中，
同时基于旧的数据版本对外提供服务。如果 slave node 开启了 AOF，那么会立即执行 BGREWRITEAOF，重写 AOF。增量复制：如果全量复制过程中，
master-slave 网络连接断掉，那么 slave 重新连接 master 时，会触发增量复制。master 直接从自己的 backlog 中获取部分丢失的数据，发送给 slave node，
默认 backlog 就是 1MB。master 就是根据 slave 发送的 psync 中的 offset 来从 backlog 中获取数据的。心跳机制：主从节点互相都会发送 heartbeat 信息。
master 默认每隔 10秒 发送一次 heartbeat，slave node 每隔 1秒 发送一个 heartbeat。异步复制：master 每次接收到写命令之后，先在内部写入数据，然后异步发送给 slave node。

#### redis高可用：
failover 故障转移，也可以叫做主备切换，master节点在故障时会自动检测，并且将某个slave节点自动切换为master节点。
#### redis哨兵模式实现高可用：
##### sentinel(哨兵)介绍：

集群监控：负责监控 redis master 和 slave 进程是否正常工作。消息通知：如果某个 redis 实例有故障，那么哨兵负责发送消息作为报警通知给管理员。
故障转移：如果 master node 挂掉了，会自动转移到 slave node 上。配置中心：如果故障转移发生了，通知 client 客户端新的 master 地址。
哨兵本身也是分布式的，故障转移时，判断一个 master node 是否宕机了，需要大部分的哨兵都同意才行，涉及到了分布式选举的问题。即使部分哨兵节点挂掉了，哨兵集群还是能正常工作的。

##### sentinel(哨兵)架构：

哨兵至少需要 3 个实例，来保证自己的健壮性。哨兵 + redis 主从的部署架构，是不保证数据零丢失的，只能保证 redis 集群的高可用性。哨兵集群必须部署 2 个以上节点，
如果哨兵集群仅仅部署了 2 个哨兵实例，quorum = 1。如果master节点宕机后，只要两个哨兵中一个认为master节点宕机，就可以进行切换，同时需要选举一个哨兵来执行故障转移，
需要满足大于一半的哨兵正常(majority)，如果有一个哨兵正好在宕机的服务器上，就无法满足majority，就无法进行故障转移。所以至少配置奇数个哨兵确保可以进行故障转移。

##### redis哨兵主备切换数据丢失问题：
情形：1.主备切换时由于是异步复制，所以可能部分数据没有复制到slave时master就宕机了，导致数据丢失。2.脑裂导致数据丢失：脑裂：某一个master节点出现网络隔离，
导致无法和slave连接，哨兵认为master节点宕机了，进行故障转移，重新选举了master节点，导致出现两个master节点。脑裂发生后有的客户端没有及时切换到新的master，
还向旧的master发送数据。当旧master恢复集群连接时，就会作为一个slave进入集群，数据进行清空，导致一部分数据丢失。

解决：设置参数：min-slaves-to-write 1，min-slaves-max-lag 10。表示至少有一个slave，数据复制和同步的延迟不能超过10秒，超过时间master就不会接收任何请求。
这要最多就会丢失10秒的数据，在可控范围内。

##### sdown 和 odown 转换机制：

sdown:主观宕机，哨兵自己认为master宕机。主要是哨兵ping master节点超过is-master-down-after-milliseconds时间就认为是宕机。
odown：客观宕机，一半以上的哨兵集群认为master宕机。哨兵在指定时间内，收到一半以上的哨兵的sdown，就认为是odown。

##### 哨兵集群的自动发现机制：
哨兵之间的通过redis的发布订阅系统(pub/sub),每个哨兵在channel里发送消息，其他的哨兵消费这个消息，并感知到其他哨兵的状态。每隔两秒，
哨兵会往自己监控的master+slave对应的channel内发送消息，消息内容包括自己的ip+host+runid+master的监控配置。每个哨兵同时会监听对应的channel，
会感知到同样监听的其他的哨兵。还会和其他哨兵交换master的监控配置，同步配置。

##### slave配置的自动纠正：

哨兵会自动纠正slave的配置，比如slave选举为master之后会确保slave复制master数据，故障转移后确保slave连接到正确的master节点上。

##### slave->master选举算法：

master节点odown之后，同时majority数量的哨兵同意主备切换之后，某个哨兵执行主备切换就会选举一个slave，考虑与master节点断开的时间，
slave的优先级，复制offset，run id。如果一个slave节点和master节点断开的时间超过down-after-milliseconds的10倍+master宕机的时间，
那么slave就不会被选举为master，同时会对slave进行排序：1.slave优先级，slave priority越低，优先级越高。2.slave priority相同，
就看replica offset，那个slave复制了越多的数据，offset越靠后，优先级越高。3.如果两个条件相同，就选择run id最小的的那个slave。

##### quorum 和 majority:

每次哨兵进行进行主备切换时，需要quorum数量的哨兵认为master odown,然后选举一个哨兵来做切换，同时哨兵需要得到majority哨兵的授权才能正式执行。

##### configuration epoch：

哨兵会对一套 redis master+slaves 进行监控，有相应的监控的配置。执行切换的那个哨兵，会从要切换到的新 master（slave->master）那里得到一个
 configuration epoch，这就是一个 version 号，每次切换的 version 号都必须是唯一的。如果第一个选举出的哨兵切换失败了，那么其他哨兵，
 会等待 failover-timeout 时间，然后接替继续执行切换，此时会重新获取一个新的 configuration epoch，作为新的 version 号。
 
##### configuration 传播：

哨兵完成切换之后，会在自己本地更新生成最新的 master 配置，然后同步给其他的哨兵，就是通过之前说的 pub/sub 消息机制。这里之前的 version 号就很重要了，
因为各种消息都是通过一个 channel 去发布和监听的，所以一个哨兵完成一次新的切换之后，新的 master 配置是跟着新的 version 号的。其他的哨兵都是根据版本号的大小来更新自己的 master 配置的。

### redis持久化：RDB和AOF(同时使用两种持久化时，重启时会使用AOF来构建数据，因为 AOF 中的数据更加完整)

#### RDB：对redis的数据执行周期性的持久化

1.RDB会生成多个数据文件，每个数据文件都代表redis某个时刻的数据。多数据文件的方式适合做冷备，将数据定期发送到远程服务器上做备份。

2.RDB对redis对外提供的读写服务影响非常小，可以让redis保持高性能，因为redis可以让主线程fork一个子线程去执行磁盘IO操作来进行RDB持久化。

3.相比较AOF，直接基于RDB数据文件来重启和恢复redis数据更快。

4.RDB备份数据是每隔5分钟或更长时间，如果想要redis故障后丢失更少的数据，RDB没有AOF好。

5.RDB在执行fork子线程生成数据文件时，如果数据文件过大，会导致客户端的服务暂停数毫秒甚至数秒。

#### AOF：记录每条写入命令为日志，以append-only的模式写入日志文件，redis重启时通过回放AOF日志中的写入命令重新构建数据

1.AOF更好的保护数据不丢失，AOF一般会每隔1秒通过后台线程执行一次fsync操作，最多丢失一秒的数据。

2.AOF 日志文件以 append-only 模式写入，所以没有任何磁盘寻址的开销，写入性能非常高，而且文件不容易破损，即使文件尾部破损，也很容易修复。

3.AOF的日志文件即使过大也不会影响客户端的读写，因为在写日志的时候，会对日志的指令进行压缩，创建一份需要恢复的数据的最小日志，
在创建新日志文件时，旧的日志文件会照常写入，当新的日志文件准备好时，交换新旧日志文件即可。

4.AOF 日志文件的命令通过非常可读的方式进行记录，这个特性非常适合做灾难性的误删除的紧急恢复。比如某人不小心用 flushall 命令清空了所有数据，
只要这个时候后台 rewrite 还没有发生，那么就可以立即拷贝 AOF 文件，将最后一条 flushall 命令给删了，然后再将该 AOF 文件放回去，就可以通过恢复机制，自动恢复所有数据。

5.对于同一份数据，AOF日志文件通常比RDB数据快照文件更大。

6.AOF 开启后，支持的写 QPS 会比 RDB 支持的写 QPS 低，因为 AOF 一般会配置成每秒 fsync 一次日志文件，当然，每秒一次 fsync，性能也还是很高的。
（如果实时写入，那么 QPS 会大降，redis 性能会大大降低）

7.以前 AOF 发生过 bug，就是通过 AOF 记录的日志，进行数据恢复的时候，没有恢复一模一样的数据出来。所以说，类似 AOF 这种较为复杂的基于命令日志 
/ merge / 回放的方式，比基于 RDB 每次持久化一份完整的数据快照文件的方式，更加脆弱一些，容易有 bug。不过 AOF 就是为了避免 rewrite 过程导致的 bug，
因此每次 rewrite 并不是基于旧的指令日志进行 merge 的，而是基于当时内存中的数据进行指令的重新构建，这样健壮性会好很多。

### redis集群：采用redis cluster，针对海量数据+高并发+高可用的场景。redis cluster支持多个master节点，每个master节点挂载多个slave节点，master节点宕机后可以选举slave节点，支持大数据量时也可以横向扩容master节点。

#### redis cluster：

1.自动将数据分片，每个master上存放一部分数据。2.提供内置的高可用支持，部分master不可用时redis可以继续运行。redis cluster的每个redis需要开放两个端口，
一个用于对外提供数据，一个用于节点间通信(cluster bus，前一个端口+10000)。用来进行故障检测，配置更新，故障转移等。cluster bus采用另一种二进制协议(gossip),
用于节点间进行高效的数据交换，占用更少的带宽和处理时间。

#### 节点间通信机制：

##### 集中式：将集群的元数据(节点信息，故障等)集中存储在某个节点上。

元数据的读取和更新时效性好，一旦元数据出现更新，就能立即更新到集中式的存储上，其他节点读取时就能感知到，但是所有的数据都集中在一个节点上，导致元数据的存储有压力。

##### gossip：所有节点都持有一份元数据，一旦元数据出现更新，就将元数据发送给集群中的其他节点，其他节点进行更新。

1.元数据比较分散，更新请求会陆续到达其他节点，减轻集群压力。但是元数据更新有延迟，导致有些节点的操作滞后。

2.gossip协议消息：meet：某个节点发送 meet 给新加入的节点，让新节点加入集群中，然后新节点就会开始与其它节点进行通信。ping：
每个节点都会频繁给其它节点发送 ping，其中包含自己的状态还有自己维护的集群元数据，互相通过 ping 交换元数据。pong：返回 ping 和 meet，
包含自己的状态和其它信息，也用于信息广播和更新。fail：某个节点判断另一个节点 fail 之后，就发送 fail 给其它节点，通知其它节点说，某个节点宕机啦。

3.ping 时要携带一些元数据，如果很频繁，可能会加重网络负担。每个节点每秒会执行 10 次 ping，每次会选择 5 个最久没有通信的其它节点。
当然如果发现某个节点通信延时达到了 cluster_node_timeout / 2，那么立即发送 ping，避免数据交换延时过长，落后的时间太长了。比如说，
两个节点之间都 10 分钟没有交换数据了，那么整个集群处于严重的元数据不一致的情况，就会有问题。所以 cluster_node_timeout 可以调节，
如果调得比较大，那么会降低 ping 的频率。每次 ping，会带上自己节点的信息，还有就是带上 1/10 其它节点的信息，发送出去，进行交换。
至少包含 3 个其它节点的信息，最多包含 总节点数减 2 个其它节点的信息。

#### 分布式寻址算法：

##### hash算法(大量缓存重建)：将key的hash值根据节点数取模，打在不同的 master 节点上。一旦有master节点宕机后，大量请求就会根据新的master节点取模，导致请求没有命中，涌入数据库。

##### 一致性hash算法：

将整个hash值空间组织成一个虚拟的圆环，将整个空间值按顺时针进行排序，将所有master节点(使用服务器的 ip 或主机名)进行hash值计算。确定在hash环上的位置。

![timewoo](https://timewoo.github.io/images/redis1.png)

redis请求的key进行hash计算之后，确定在hash环上的位置，从此位置沿环顺时针“行走”，遇到的第一个 master 节点就是 key 所在位置。

![timewoo](https://timewoo.github.io/images/redis2.png)

在一致性hash算法中，如果新增一个节点或者删除一个节点，受影响的数据仅仅是此节点到环空间前一个节点（沿着逆时针方向行走遇到的第一个节点）之间的数据，其它不受影响。具有较好的容错性和可扩展性。

![timewoo](https://timewoo.github.io/images/redis3.png)

![timewoo](https://timewoo.github.io/images/redis4.png)

但是，一致性hash算法在节点较少时，容易出现因为节点分布不均而造成热点缓存问题。

![timewoo](https://timewoo.github.io/images/redis5.png)

为了解决这种热点缓存的问题，一致性hash算法引入虚拟节点机制，即一个节点计算多个hash值，每个位置放一个虚拟节点。实现了数据的均匀分布，负载均衡。

![timewoo](https://timewoo.github.io/images/redis6.png)

##### redis cluster的hash slot算法：

redis cluster 有固定的 16384 个 hash slot，对每个 key 计算 CRC16 值，然后对 16384 取模，可以获取 key 对应的 hash slot。

redis cluster 中每个 master 都会持有部分 slot，比如有 3 个 master，那么可能每个 master 持有 5000 多个 hash slot。hash slot 让 node 的增加和移除很简单，
增加一个 master，就将其他 master 的 hash slot 移动部分过去，减少一个 master，就将它的 hash slot 移动到其他 master 上去。移动 hash slot 的成本是非常低的。
客户端的 api，可以对指定的数据，让他们走同一个 hash slot，通过 hash tag 来实现。

任何一台机器宕机，另外两个节点，不影响的。因为 key 找的是 hash slot，不是机器。

#### redis cluster高可用和主备切换：
##### 判断节点宕机：

如果一个节点认为另外一个节点宕机，那么就是 pfail，主观宕机。如果多个节点都认为另外一个节点宕机了，那么就是 fail，客观宕机，跟哨兵的原理几乎一样，sdown，odown。

在 cluster-node-timeout 内，某个节点一直没有返回 pong，那么就被认为 pfail。

如果一个节点认为某个节点 pfail 了，那么会在 gossip ping 消息中，ping 给其他节点，如果超过半数的节点都认为 pfail 了，那么就会变成 fail。

##### 从节点过滤：

对宕机的 master node，从其所有的 slave node 中，选择一个切换成 master node。

检查每个 slave node 与 master node 断开连接的时间，如果超过了 cluster-node-timeout * cluster-slave-validity-factor，那么就没有资格切换成 master。

##### 从节点选举：

每个从节点，都根据自己对 master 复制数据的 offset，来设置一个选举时间，offset 越大（复制数据越多）的从节点，选举时间越靠前，优先进行选举。

所有的 master node 开始 slave 选举投票，给要进行选举的 slave 进行投票，如果大部分 master node（N/2 + 1）都投票给了某个从节点，那么选举通过，那个从节点可以切换成 master。

从节点执行主备切换，从节点切换为主节点。

##### 与哨兵比较：

整个流程跟哨兵相比，非常类似，所以说，redis cluster 功能强大，直接集成了 replication 和 sentinel 的功能。

### redis实际工作中出现的问题
#### redis雪崩、穿透和击穿
##### redis雪崩：

缓存雪崩是指在某一时刻内缓存失效，可能是缓存服务器宕机或者缓存时间设置一致，旧缓存失效，新缓存未生成，导致大量请求全部查询数据库，数据库压力过大宕机。

解决：1.事前：redis高可用，主从+哨兵，redis cluster，避免缓存服务器全部崩溃。2.事中：本地ehcache缓存+hystrix限流和降级，
避免所有请求访问数据库导致数据库宕机。本地ehcache是为了在缓存服务器完全不可用的情形下，可以支撑一部分的请求，一般是将缓存数据同时写入redis和本地ehcache中，
访问时先查本地ehcache，再从redis中查询。hystrix限流和降级可以将请求限制在数据库可以承受的数量，这样就可以保证数据库不会宕机，可以处理部分请求，系统不会完全挂掉。
3.事后：redis持久化，能够快速恢复数据，尽量保证数据不丢失。

##### redis穿透：

缓存穿透是指大部分的请求数据在缓存和数据库中都无法查询到，导致大部分请求都去请求数据库，导致数据库压力增大。

解决：1.缓存空值，即将不存在的数据缓存一个空值到缓存中，并且设置一个过期时间，这样同样的请求数据就直接取缓存的数据，不会请求数据库。

2.BloomFilter(布隆过滤器)：它实际上是一个很长的二进制向量和一系列随机映射函数。布隆过滤器可以用于检索一个元素是否在一个集合中。
它的优点是空间效率和查询时间都比一般的算法要好的多，缺点是有一定的误识别率和删除困难。它判断一个元素不存在那肯定就是不存在，
它判断存在的时候有一定误差，是有可能不存在的。所以对海量数据的查重非常高效，可以用来解决缓存穿透的问题，在缓存的前面加一层BloomFilter，
将数据库的数据导入，这样请求先经过BloomFilter，存在就去查询缓存，不存在直接返回。

##### redis击穿：

缓存击穿是指缓存中的部分数据属于热点数据，在这部分数据失效的瞬间大量的请求访问，请求击穿缓存到达数据库，导致数据库压力增大。

解决：1.若缓存的数据基本不更新，可以尝试将缓存过期时间设为永不过期。2.若缓存数据更新频率低并且缓存重新生成流程时间较短，
可以采用redis或Zookeeper的互斥锁来确保少部分请求查询数据库并且重新生成缓存数据，其余请求在锁释放后可以直接从缓存中获取数据。
3.若缓存数据更新频繁并且缓存重新生成时间较长，可以启用定时线程定时缓存快要过期的缓存数据，保证所有请求都能够访问缓存。

### redis和数据库的双写一致：

如果数据库和缓存不要求强一致性，即缓存和数据库的数据实时保证一致，就不要做读写请求串行，通过队列保证一致性，因为串行可以保证数据强一致性，
但是会导致系统的数据吞吐量大幅降低，用比正常情况下多几倍的机器去支撑线上的一个请求。

#### 缓存更新策略：Cache Aside Pattern

读数据时先读缓存，缓存没有就读数据库，然后将数据放入缓存。更新时先更新数据库然后再删除缓存。删除缓存而不是更新缓存是由于缓存更新的场景比较复杂，
比如缓存涉及多个数据，某个数据更新之后，需要和其他数据的进行逻辑计算后才能获取最新的缓存数据，如果计算复杂，同时更新频繁，这样更新缓存的效率就很低，
其实删除缓存，而不是更新缓存，就是一个 lazy 计算的思想，不要每次都重新做复杂的计算，不管它会不会用到，而是让它到需要被使用的时候再重新计算。

#### 缓存和数据库不一致情况：

1.数据库更新成功，缓存删除失败：

解决：先删除缓存，再更新数据库，这样无论数据库时候更新成功，缓存中的数据都是和数据库一致的。

2.数据库再更新的中途有请求访问，缓存存储旧数据，数据库更新为新数据(高并发)：

解决：将同一个数据的读写请求串行到一个jvm队列中，即根据数据的唯一标识，将读写操作发送到同一个jvm队列中，如果在进行写操作时，
有线程同时进行读请求，如果没有读到缓存，就将操作发送到队列中，等待写请求完成后再去执行读请求。一个队列中多个更新缓存请求串在一起是没意义的，
因此可以做过滤，如果发现队列中已经有一个更新缓存的请求了，那么就不用再放个更新请求操作进去了，直接等待前面的更新操作请求完成即可。
同时要设置请求的超时时间，超时直接从数据库取旧值，避免请求长时间阻塞。高并发下需要避免读请求长时间阻塞，数据更新频繁情况下会导致更新请求堆积，
导致读请求阻塞。所以一般都是加服务器，让队列处理更少的操作。

### redis并发竞争：多个redis客户端去更新同一个值导致数据最终的结果不一致

解决：1.分布式锁+时间戳，分布式锁时为了把并行操作改为串行，避免资源竞争，可以使用redis自带的分布式锁或者Zookeeper的分布式锁，
时间戳是指在数据修改时存储当前时间戳，对同一个数据操作时判断当前时间戳和缓存中的时间戳，从而使高并发下的数据达到最终的一致性。

2.利用消息队列将redis的操作串行化。

### 分布式缓存数据一致性

![timewoo](https://timewoo.github.io/images/cache.png)

### redis主流java客户端框架

Jedis：是Redis的java实现客户端，提供较为全面的Redis命令支持。比较全面的提供了Redis的操作特性。使用阻塞的I/O，方法的调用都是同步的，程序流需要等到
sockets处理完I/O才能执行，不支持异步。Jedis客户端实例不是线程安全的，所以需要通过连接池来使用Jedis。

Redisson：实现了分布式和可扩展的java数据结构。促使使用者对Redis的关注分离，提供了许多分布式相关的操作服务，比如分布式锁，分布式集合，可通过Redis
支持延迟队列。基于Netty框架的事件驱动的通信层，方法的调用都是异步的，Redisson的API是线程安全的，所以可以操作单个Redisson连接来完成各种操作。

Lettuce：高级Redis客户端，用于线程安全同步，异步和响应使用，支持集群，Sentinel，管道和编码器。主要在分布式缓存框架上使用较多。基于Netty框架的事件
驱动的通信层，方法的调用都是异步的，Lettuce的API是线程安全的，所以可以操作单个Lettuce连接来完成各种操作。

![timewoo](https://timewoo.github.io/images/redis7.png)

由于Jedis使用同步和阻塞的方式，而且Jedis是线程不安全的，所以在性能上会比Redisson和Lettuce低，springboot 2.x以上整合的spring-boot-starter-data-redis
默认使用的redis的客户端是Lettuce。如果要使用Jedis和Redisson则需要自己引入相关依赖。Jedis提供了比较全面的Redis命令支持，调用的是比较底层的Redis API，
基本上和Redis自身的API一致，但是Jedis不支持Redis的高级功能，比如分布式集合，分布式调度任务，集群pipeline，事务和延迟队列等，而Redisson基本支持Redis的
高级功能，同时是异步的操作，而且Redisson可以实现分布式锁，但是Redisson基本上都是对集合进行操作，对字符串操作支持很差，因此在平常使用redis的存储数据功能时使用
Lettuce即可，若要使用Redis高级功能则可以引入Redisson来操作，Redisson在分布式锁的操作上还实现了分布式锁的续期功能，即在分布式加锁的时间内由于业务执行超时导致
锁释放，然后被别的线程获取锁，出现锁被多个线程持有，然后前一个释放锁时将后面的线程持有的锁释放了，导致锁失效。Redisson为了解决这个问题在加锁成功后会注册一个定时任务监听锁，
每隔10s去检查线程是否持有锁，如果还持有锁则会对锁进行续期，默认是续期30s。由于Redisson主要是封装了Redis中的分布式特性，所以Redisson这些操作的底层都是将逻辑封装成lua脚本来保证原子性。

### 分布式锁

主流的分布式锁的实现主要是用Redis和Zookeeper来实现。

Redis在单节点的下主要使用SETNX命令，即通过执行SET key value [EX seconds] [PX milliseconds] NX命令创建key来进行加锁操作，NX表示只有当key不存在时才会创建成功，
否则返回失败，EX seconds和PX milliseconds都是设置key的过期时间，只是EX是秒级的，PX是毫秒级的，只能设置为PX或EX中的一种。使用SETNX主要是通过lua脚本保证SET和EX命令的原子性，
避免EX命令未执行导致没有设置过期时间造成死锁。然后在删除时通过lua脚本判断删除的key和设置的key是否相同，相同则删除。
```
-- 删除锁的时候，找到 key 对应的 value，跟自己传过去的 value 做比较，如果是一样的才删除。
if redis.call("get",KEYS[1]) == ARGV[1] then
    return redis.call("del",KEYS[1])
else
    return 0
end
```
key除了要设置成唯一之外还需要添加随机数，因为在获取锁的过程中由于业务执行时间过长，导致锁释放，然后被另一个线程获取，然后前一个线程释放时会将后一个锁一同释放，
因此需要添加随机数来保证获取和释放的是同一个锁，防止锁误解除。对于业务执行时间过长导致锁被多个线程持有问题可以通过锁的续期来解决。因此在redis单节点的情况下使用
SETNX命令可以实现分布式锁，但是在redis集群环境下出现主备切换和脑裂的情况就会出现问题，主备切换时主节点上的key未同步到从节点，切换完成后其他线程也可以获取到锁。
脑裂时由于网络原因导致出现多个主节点，导致不同线程可以获取不同主节点上的相同key。为了解决redis集群中出现的分布式锁问题，redis官方推出了RedLock算法，RedLock
算法的主要逻辑在大部分主节点上获取锁成功则加锁成功。假设redis集群的主节点有N个，客户端在获取锁时首先获取本地当前时间startTime，startTime是毫秒级别的。然后客户端
会尝试在N个主节点中执行单节点设置锁的操作，客户端会设置获取key的超时时间，一般超时时间是小于key有效时间，比如key设置的有效时间expireTime是10s，则超时时间一般是5-50ms，
设置超时时间是防止客户端和有故障的redis节点长时间通信造成超过key的有效时间，当某个节点不可用时应该尽快尝试和下个节点进行通信。当客户端和所有节点通信完成后会获取本地当前
时间endTime，然后计算和所有节点通信所花费的时间(endTime-startTime)，然后客户端会判断只有在超过半数的节点上(n/2+1)设置key成功,并且花费时间(endTime-startTime)
小于有效时间expireTime时才会认为获取到锁，如果获取到锁，客户端会将锁的有效时间设置为(expireTime-(endTime-startTime))。如果客户端因为无法满足半数以上节点
获取锁或者有效时间为负数则会认为获取锁失败，则会在所有节点进行锁的释放，不管节点是否获取到锁，这样可以避免因为节点通信问题导致节点上已加锁成功而没有释放的问题。未
获取到锁的线程会不断轮询去尝试获取锁。RedLock采用半数以上加锁成功来判断获取锁可以保证其他客户端无法获取锁，当存在n/2+1节点上设置key之后，其他客户端就无法在n/2+1上设置key，
由于客户端需要对节点依次加锁，加锁操作存在时间差，尽管每个节点设置的key的过期时间一样，但是key的实际到期时间是有先后的，所以需要将锁的有效时间设置成expireTime-(endTime-startTime)，
来保证在redis key的过期时间是比锁的有效时间长，避免节点key比锁先失效。在RedLock推出后，分布式系统的专家马丁·克莱普曼博士(Martin Kleppmann)对RedLock进行了一些质疑，
虽然RedLock算法解决了redis集群环境中多节点获取分布式锁的问题，但是还是没有解决客户端持有锁的时间过长导致redis key失效，然后导致多个客户端获取到同一个锁进行操作，
数据就会出现问题。Martin提出为锁添加一个token-fencing，一个递增的token，每个客户端加锁时分配一个token，提交数据时需要判断token是否比上一次token大，是则提交，
不是则拒绝，这样出现阻塞时导致多个客户端都获取锁时，只有后面的客户端才能提交。还有RedLock是一种对系统时间强依赖的分布式锁算法，当节点间的系统时间不一致时就会导致锁被多端持有，
比如ABC三个节点和DE两个客户端，D获取AB节点的key，B节点的系统时间向前跳跃导致key提前过期，E获取BC节点的key，这样就导致DE都认为获取到锁。
因此RedLock是对redis节点的系统时间有强依赖的。而且一般认为好的分布式算法应该是异步的，由于分布式中网络通信可能延长任意时间，系统时间也可能变化，
分布式算法在出现这种情况时最多不能立即返回正确的结果，但是不能返回错误结果。比如Paxos算法，所以Martin任务RedLock这种强依赖系统时间的算法是不安全的，
Martin指出锁主要分为两种用途，一种是为了效率，保证任务只执行，但是锁偶尔失效只是加重一些计算，没有其他影响。一种是正确第一，任何情况下都不允许锁失效，
保证数据的一致性。因此Martin认为在为了效率的情况下可以使用单节点redis加锁操作儿没有必要使用RedLock，毕竟RedLock需要创建多个redis实例，
在为了正确的情况下RedLock又是不安全的分布式锁算法，认为是一个糟糕的选择。对于Martin的质疑，RedLock的作者Antirez给予了回应，对于第一个token-fencing机制，
Antirez认为如果服务器能够提供fencing这样保证互斥的机制，那就没必要选择分布式锁，也不需要要求分布式锁的提供这么强安全性。而且RedLock虽然不能提供递增的
token-fencing，但是可以提供随机字符串，通过CAS操作同样可以实现互斥。对于第二个系统时间依赖问题，Antirez认为主要RedLock主要失效来源于时钟跳跃，长时间
的GC pause和长时间的网络延迟。对于后两种场景其实RedLock已经有处理，在系统时间正确的情况下，RedLock会判断获取锁的时间是否超过有效时间，这样就避免了获取
到已经失效的锁。对于第一个时钟跳跃问题Antirez认为这种场景一般出现在人为修改和系统同步时间出问题，都是可以通过运维去处理的。所以在选用分布式锁的时候需要对算法
存在的问题进行理解才能更好的选择。

Martin推荐使用Zookeeper来实现分布式锁，Zookeeper主要是通过建立临时节点来获取锁，Zookeeper的节点是唯一的，其他客户端会获取锁失败，解锁时只需要删除节点
即可，而且Zookeeper的临时节点在客户端崩溃后会自动删除，避免了死锁的产生。同时Zookeeper的Watch机制能够让其他未获取到锁的客户端进行阻塞获取锁，而不是直接
返回获取锁失败，其他客户端可以在创建节点失败后注册一个监听器，释放锁时可以通知客户端，等待的客户端就可以重新加锁。还有一种方法时创建临时顺序节点，每个客户端创建
临时顺序节点，然后判断节点是否时最小节点，如果是则获取锁，不是则在前一个节点上注册监听器，前一个节点释放后通知后一个客户端执行，客户端按照节点顺序获取锁。相比较
redis的分布式锁，zookeeper的分布式锁不需要轮询获取锁的状态，只需要等待监听器的通知回调执行，性能开销小。而且redis分布式锁在客户端崩溃后需要等待key过期后才能
获取锁，而Zookeeper的临时节点在客户端崩溃后自动删除。




